{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_training.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNdYSb7sDR3hX+nA6J4tD1c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeefan1999/Explainable-Health-Prediction-with-Transfer-Learning/blob/main/Colab/model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i_mZEpFv5Pg",
        "outputId": "63f29262-62ca-4caa-9176-95e827847c0c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8dUoxU7v72c"
      },
      "source": [
        "import os,sys,inspect\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import ZeroPadding2D,GlobalAveragePooling2D,Dense,Conv2D, Convolution2D, Flatten, Dropout, MaxPooling2D, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import Input\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.metrics import accuracy_score \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from keras import applications\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import keras\n",
        "import sklearn.metrics as metrics\n",
        "import seaborn as sns\n",
        "from keras_vggface.vggface import VGGFace"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg_ZTpIzwxV3"
      },
      "source": [
        "import random\n",
        "from scipy import ndimage\n",
        "\n",
        "def load_data(datadir,categories):\n",
        "    \n",
        "    datalength = 0\n",
        "    data = list()\n",
        "    labels = list()\n",
        "    for i,category in enumerate(categories):\n",
        "        path = os.path.join(datadir, category)\n",
        "        path_list = os.listdir(path)\n",
        "        if ('.DS_Store') in path_list:\n",
        "            path_list.remove('.DS_Store')\n",
        "        for img in path_list:\n",
        "            img_ = os.path.join(path,img)\n",
        "            img_ = cv2.imread(img_)\n",
        "            img_ = cv2.cvtColor(img_,cv2.COLOR_BGR2RGB)\n",
        "            data.append(img_)\n",
        "\n",
        "    return np.asarray(data)\n",
        "\n",
        "def get_label_augmented_data(symptom,normal):\n",
        "    \"\"\"return train data and label\"\"\"\n",
        "    data_normal = list()\n",
        "    labels = list()\n",
        "    data_symptoms = list()\n",
        "    for image in normal:\n",
        "        image2 = tf.image.flip_left_right(image)\n",
        "        data_normal.append(np.asarray(image))\n",
        "        data_normal.append(np.asarray(image2))\n",
        "        data_normal.append(np.asarray(tf.image.rot90(image,tf.random.uniform(shape=[],\n",
        "                                                                              minval=1,maxval=4,\n",
        "                                                                             dtype=tf.int32))))\n",
        "        data_normal.append(np.asarray(tf.image.random_brightness(image,max_delta=0.3)))\n",
        "        image3 = ndimage.rotate(image,15)\n",
        "        kernel = np.ones((5,5),np.float32)/25\n",
        "        dst = cv2.filter2D(image,-1,kernel)\n",
        "        data_normal.append(image3)\n",
        "        data_normal.append(dst)\n",
        "\n",
        "    for image in symptom:\n",
        "        image2 = tf.image.flip_left_right(image)\n",
        "        data_symptoms.append(np.asarray(image))\n",
        "        data_symptoms.append(np.asarray(image2))\n",
        "        data_symptoms.append(np.asarray(tf.image.rot90(image,tf.random.uniform(shape=[],\n",
        "                                                                              minval=1,maxval=4,\n",
        "                                                                             dtype=tf.int32))))\n",
        "        data_symptoms.append(np.asarray(tf.image.random_brightness(image,max_delta=0.3)))\n",
        "        image3 = ndimage.rotate(image,15)\n",
        "\n",
        "        kernel = np.ones((5,5),np.float32)/25\n",
        "        dst = cv2.filter2D(image,-1,kernel)\n",
        "        data_symptoms.append(image3)\n",
        "        data_symptoms.append(dst)\n",
        "        \n",
        "    train_data = np.concatenate((np.array(data_normal),\n",
        "                       np.array(data_symptoms)))\n",
        "    labels = np.concatenate((np.zeros(6000),np.ones(3600)))\n",
        "        \n",
        "    return train_data,labels\n",
        "\n",
        "\n",
        "def haar_crop():\n",
        "\n",
        "    path_symptoms = \"Train/Symptoms_ori\"\n",
        "    path_normal = \"Train/Normal_ori\"\n",
        "\n",
        "    savepath_symptoms = \"Train/Symptoms_crop/\"\n",
        "    savepath_normal = \"Train/Normal_crop/\"\n",
        "\n",
        "    facecascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
        "\n",
        "    found_faces = list()\n",
        "\n",
        "    dirs = os.listdir(path_symptoms)\n",
        "\n",
        "    if('.DS_Store' in dirs):\n",
        "        dirs.remove('.DS_Store')\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    for item in dirs:\n",
        "        if os.path.isfile(path_symptoms+'/'+item):\n",
        "            image = cv2.imread(path_symptoms+'/'+item)\n",
        "            gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "            faces = facecascade.detectMultiScale(\n",
        "                    gray,\n",
        "                    scaleFactor=1.3,\n",
        "                    minNeighbors=3,\n",
        "                    minSize=(30,30))\n",
        "            if(len(faces)==1):\n",
        "                found_faces.append(item)\n",
        "                for (x,y,w,h) in faces:\n",
        "                    cv2.rectangle(image,(x,y),(x+w, y+h), (0,255,0),2)\n",
        "                    roi_color = image[y+2:y+h-2, x+2:x+w-2]\n",
        "                    cv2.imwrite(savepath_symptoms+item, roi_color)\n",
        "\n",
        "\n",
        "    found_faces = list()\n",
        "\n",
        "    dirs = os.listdir(path_normal)\n",
        "\n",
        "    if('.DS_Store' in dirs):\n",
        "        dirs.remove('.DS_Store')\n",
        "\n",
        "    i = 0\n",
        "\n",
        "    for item in dirs:\n",
        "        if os.path.isfile(path_normal+'/'+item):\n",
        "            image = cv2.imread(path_normal+'/'+item)\n",
        "            gray = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "            faces = facecascade.detectMultiScale(\n",
        "                    gray,\n",
        "                    scaleFactor=1.3,\n",
        "                    minNeighbors=3,\n",
        "                    minSize=(30,30))\n",
        "            if(len(faces)==1):\n",
        "                found_faces.append(item)\n",
        "                for (x,y,w,h) in faces:\n",
        "                    cv2.rectangle(image,(x,y),(x+w, y+h), (0,255,0),2)\n",
        "                    roi_color = image[y+2:y+h-2, x+2:x+w-2]\n",
        "                    cv2.imwrite(savepath_normal+item, roi_color)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def resize_rename():\n",
        "\tpath_symptoms = \"Train/Symptoms_crop/\"\n",
        "\tpath_normal = \"Train/Normal_crop/\"\n",
        "\tsave_path = \"Train/Normal/\"\n",
        "\ti = 0\n",
        "\tdirs = os.listdir(path_symptoms)\n",
        "\tdirs.remove('.DS_Store')\n",
        "\tfor item in dirs:\n",
        "\t    if os.path.isfile(path_symptoms+item):\n",
        "\t        image = Image.open(path_symptoms+item)\n",
        "\t        image_resized = image.resize((200,200))\n",
        "\t        image_resized.save(save_path+str(i)+'_symptoms.jpg','PNG', quality=100)\n",
        "\t        i+=1\n",
        "\n",
        "\tsave_path = \"Train/Symptoms/\"\n",
        "\ti = 0\n",
        "\tdirs = os.listdir(path_normal)\n",
        "\tdirs.remove('.DS_Store')\n",
        "\tfor item in dirs:\n",
        "\t    if os.path.isfile(path_normal+item):\n",
        "\t        image = Image.open(path_normal+item)\n",
        "\t        image_resized = image.resize((200,200))\n",
        "\t        image_resized.save(save_path+str(i)+'_normal.jpg','PNG', quality=100)\n",
        "\t        i+=1\n",
        "    \n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x66VeiRMxClZ"
      },
      "source": [
        "data_normal = load_data('/content/drive/My Drive/Augmentation Data',['Normal'])\n",
        "data_symptoms = load_data('/content/drive/My Drive/Augmentation Data',['Symptoms'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHekeXJ9-sLK"
      },
      "source": [
        "train_data = np.concatenate((np.array(data_normal),\n",
        "                           np.array(data_symptoms)))\n",
        "labels = np.concatenate((np.zeros(6000),np.ones(3600)))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZKw7Rqd-yxp"
      },
      "source": [
        "## VGGFace\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsqxV1R5_AM2",
        "outputId": "e35af6b9-51ac-4452-cc70-3dd4afad872b"
      },
      "source": [
        "model = VGGFace(weights='vggface',\n",
        "                        include_top=False,\n",
        "                        input_shape=(200,200,3))\n",
        "\n",
        "def create_features(dataset,model):\n",
        "    \n",
        "    train_data = np.expand_dims(dataset,axis=0)\n",
        "    train_data = np.vstack(train_data)\n",
        "    features = model.predict(train_data)\n",
        "    features_flatten = features.reshape((features.shape[0],6*6*512))\n",
        "    \n",
        "    return train_data, features, features_flatten\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_vgg16.h5\n",
            "58916864/58909280 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnj4Z9Dd_EWX"
      },
      "source": [
        "train_data, features, features_flatten = create_features(train_data,model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dSxpJmG_FnO"
      },
      "source": [
        "X_train, X_valid, y_train,y_valid = train_test_split(features,labels,test_size=0.2,random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train,y_train,test_size=0.15,random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNdjGGLz_JI4"
      },
      "source": [
        "checkpointer = ModelCheckpoint(filepath='/content/drive/My Drive/cnn_best.hdf5',verbose=1,\n",
        "                              save_best_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WrlOy-F_KmM"
      },
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=2, patience=10)\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(6, 6, 512))) \n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu',padding=\"Same\")) \n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mmxnawo_Mry"
      },
      "source": [
        "model_history = model.fit(X_train,y_train,\n",
        "                         epochs=100,\n",
        "                         callbacks=[es,checkpointer],\n",
        "                         verbose=2,\n",
        "                         validation_data=(X_valid,y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKFCZnsh_OKV"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDVNb1Xh_OfT"
      },
      "source": [
        "print(model.evaluate(X_test,y_test))\n",
        "predictions_ = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Md11zGE_TLv"
      },
      "source": [
        "predictions = np.argmax(predictions_, axis=1)\n",
        "\n",
        "print(classification_report(y_test,predictions))\n",
        "print(\"Accuracy:\",accuracy_score(y_test,predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3FwYtgK_Uqg"
      },
      "source": [
        "cnf_matrix = metrics.confusion_matrix(y_test, predictions)\n",
        "cnf_matrix\n",
        "\n",
        "cm = metrics.confusion_matrix(y_test, predictions)\n",
        "score = accuracy_score(y_test,predictions)\n",
        "\n",
        "plt.figure(figsize=(9,9))\n",
        "ax = sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, \n",
        "                 square=True,cmap='Blues_r')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "all_sample_title = 'Accuracy Score : {0}'.format(score)\n",
        "plt.title(all_sample_title, size=15)\n",
        "\n",
        "bottom, top = ax.get_ylim()\n",
        "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}